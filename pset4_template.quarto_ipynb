{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PS4: Spatial Analysis of Rural Hospital Closures by Attaullah Abbasi (attaullahabbasi12)\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "\n",
        "**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. \n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "    \n",
        "## Style Points (10 pts) \n",
        "Please refer to the minilesson on code style\n",
        "**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.\n",
        "\n",
        "## Submission Steps (10 pts)\n",
        "1. This problem set is a paired problem set.\n",
        "2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.\n",
        "    - Partner 1 (Attaullah Abbasi and attaullahabbasi):\n",
        "    - Partner 2 (N/A - assignment completed solo):\n",
        "3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. \n",
        "4. \"This submission is our work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: AA\n",
        "5. \"I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  (1 point): AA\n",
        "6. Late coins used this pset: 1 Late coins left after submission: 1\n",
        "7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, \n",
        "    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. \n",
        "8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.\n",
        "9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.\n",
        "10. (Partner 1): tag your submission in Gradescope\n",
        "\n",
        "**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. \n",
        "\n",
        "## Download and explore the Provider of Services (POS) file (10 pts)\n",
        "\n",
        "1. For the 2016 data, I pulled the following variables:\n",
        "\n",
        "    i. `PRVDR_NUM`: Unique CMS certification number for each hospital\n",
        "    ii. `FAC_NAME`: Facility name\n",
        "    iii. `PRVDR_CTGRY_CD`: Provider category, to identify hospitals\n",
        "    iv. `PRVDR_CTGRY_SBTYP_CD`: Provider subtype, to identify short-term hospitals\n",
        "    v. `CRTFCTN_DT`: Certification date, to verify active facilities\n",
        "    vi. `ORGNL_PRTCPTN_DT`: Original participation date, to confirm participation history\n",
        "    vii. `PGM_TRMNTN_CD`: Termination code, to track closures\n",
        "    viii. `TRMNTN_EXPRTN_DT`: Termination date, to identify closure timelines\n",
        "    ix. `ZIP_CD`: Zip code for geographic analysis\n",
        "    x. `STATE_CD`: State abbreviation, useful for filtering Texas and nearby states\n",
        "    xi. `CITY_NAME`: City name for additional geographic context\n",
        "    xii. `ST_ADR`: Street address for potential geolocation\n",
        "    xiii. `CBSA_URBN_RRL_IND`: Urban-rural indicator, helpful for understanding location context\n",
        "    xiv. `GNRL_CNTL_TYPE_CD`: Control type, to analyze hospital ownership\n",
        "\n",
        "These variables allow for filtering, spatial analysis, tracking closures, and understanding hospital characteristics as required by the problem set.\n",
        "\n",
        "\n",
        "2. \n"
      ],
      "id": "7dc1c2ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the pos2016.csv file\n",
        "data_2016 = pd.read_csv(\"pos2016.csv\")\n",
        "\n",
        "# Filter for short-term hospitals with provider type code 01 and subtype code 01\n",
        "short_term_hospitals_2016 = data_2016[(data_2016['PRVDR_CTGRY_CD'] == 1) & (data_2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]\n",
        "\n",
        "# a. Count the number of hospitals in this subset\n",
        "hospital_count_2016 = short_term_hospitals_2016.shape[0]\n",
        "hospital_count_2016"
      ],
      "id": "672eda09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    a.\n",
        "The dataset reports 7,245 short-term hospitals for 2016.\n",
        "\n",
        "This count seems high compared to typical figures from industry sources, such as the American Hospital Association (AHA), which might suggest differences in data scope or classification criteria used by CMS.\n",
        "\n",
        "    b.\n",
        "According to the American Hospital Association (AHA), there were approximately 5,534 registered hospitals in the U.S. in 2016, with around 4,840 classified as community hospitals, which includes most short-term general hospitals. This figure is notably lower than the 7,245 short-term hospitals reported in the CMS dataset.\n",
        "\n",
        "Reasons for the Discrepancy:\n",
        "\n",
        "Data Scope: The CMS dataset may include facilities that are Medicare/Medicaid-certified but not registered with the AHA, potentially increasing the count.\n",
        "\n",
        "Different Definitions: CMS may classify certain specialty hospitals or facilities providing limited services as short-term if they meet Medicare eligibility criteria, even if other sources don’t typically include them.\n",
        "\n",
        "Timing and Updates: The dataset represents a Q4 2016 snapshot, while the AHA’s data may reflect closures, mergers, or reclassifications over the entire year.\n",
        "\n",
        "These factors could explain the higher count of short-term hospitals in the CMS dataset compared to other industry sources.\n",
        "\n",
        "3. \n"
      ],
      "id": "dc28305c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load each year's data with appropriate encoding where needed\n",
        "data_2016 = pd.read_csv(\"pos2016.csv\")\n",
        "data_2017 = pd.read_csv(\"pos2017.csv\")\n",
        "data_2018 = pd.read_csv(\"pos2018.csv\", encoding=\"ISO-8859-1\")\n",
        "data_2019 = pd.read_csv(\"pos2019.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "# Define a function to filter for short-term hospitals\n",
        "def filter_short_term(data):\n",
        "    return data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]\n",
        "\n",
        "# Apply filtering to each dataset and add a 'Year' column for each\n",
        "data_2016 = filter_short_term(data_2016)\n",
        "data_2016['Year'] = 2016\n",
        "\n",
        "data_2017 = filter_short_term(data_2017)\n",
        "data_2017['Year'] = 2017\n",
        "\n",
        "data_2018 = filter_short_term(data_2018)\n",
        "data_2018['Year'] = 2018\n",
        "\n",
        "data_2019 = filter_short_term(data_2019)\n",
        "data_2019['Year'] = 2019\n",
        "\n",
        "# Combine all datasets into a single DataFrame\n",
        "all_years_data = pd.concat([data_2016, data_2017, data_2018, data_2019])\n",
        "\n",
        "# Count the number of observations (hospitals) by year\n",
        "observations_by_year = all_years_data.groupby('Year').size().reset_index(name='Count')\n",
        "\n",
        "# Plot using Altair\n",
        "bars = alt.Chart(observations_by_year).mark_bar(color='steelblue').encode(\n",
        "    x=alt.X('Year:O', title='Year'),\n",
        "    y=alt.Y('Count:Q', title='Number of Short-Term Hospitals')\n",
        ").properties(\n",
        "    title='Number of Observations in Dataset by Year',\n",
        "    width=400,\n",
        "    height=300\n",
        ")\n",
        "\n",
        "# Add text labels on top of each bar\n",
        "text = bars.mark_text(\n",
        "    align='center',\n",
        "    baseline='bottom',\n",
        "    dy=-5  # Adjust the position of the text above the bars\n",
        ").encode(\n",
        "    text='Count:Q'\n",
        ")\n",
        "\n",
        "# Combine the bars and the text\n",
        "(bars + text).display()"
      ],
      "id": "c83f421f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n",
        "    a.\n"
      ],
      "id": "d6676b6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Assuming data for each year is already filtered and loaded\n",
        "# If not, make sure data_2016, data_2017, data_2018, data_2019 are loaded and filtered as before\n",
        "\n",
        "# Add a 'Year' column to each dataset\n",
        "data_2016['Year'] = 2016\n",
        "data_2017['Year'] = 2017\n",
        "data_2018['Year'] = 2018\n",
        "data_2019['Year'] = 2019\n",
        "\n",
        "# Combine all datasets into a single DataFrame\n",
        "all_years_data = pd.concat([data_2016, data_2017, data_2018, data_2019])\n",
        "\n",
        "# Count unique hospitals (by PRVDR_NUM) per year\n",
        "unique_hospitals_by_year = all_years_data.groupby('Year')['PRVDR_NUM'].nunique().reset_index(name='UniqueCount')\n",
        "\n",
        "# Plot using Altair\n",
        "bars = alt.Chart(unique_hospitals_by_year).mark_bar(color='seagreen').encode(\n",
        "    x=alt.X('Year:O', title='Year'),\n",
        "    y=alt.Y('UniqueCount:Q', title='Number of Unique Short-Term Hospitals')\n",
        ").properties(\n",
        "    title='Number of Unique Short-Term Hospitals in Dataset by Year',\n",
        "    width=400,\n",
        "    height=300\n",
        ")\n",
        "\n",
        "# Add text labels on top of each bar\n",
        "text = bars.mark_text(\n",
        "    align='center',\n",
        "    baseline='bottom',\n",
        "    dy=-5  # Adjust the position of the text above the bars\n",
        ").encode(\n",
        "    text='UniqueCount:Q'\n",
        ")\n",
        "\n",
        "# Display the combined chart with bars and labels\n",
        "(bars + text).display()\n",
        "\n",
        "\n",
        "# Double checking for duplicates\n",
        "# Check for duplicates in PRVDR_NUM within each year\n",
        "duplicates_2016 = data_2016.duplicated(subset='PRVDR_NUM').sum()\n",
        "duplicates_2017 = data_2017.duplicated(subset='PRVDR_NUM').sum()\n",
        "duplicates_2018 = data_2018.duplicated(subset='PRVDR_NUM').sum()\n",
        "duplicates_2019 = data_2019.duplicated(subset='PRVDR_NUM').sum()\n",
        "\n",
        "print(f\"2016 Duplicates: {duplicates_2016}\")\n",
        "print(f\"2017 Duplicates: {duplicates_2017}\")\n",
        "print(f\"2018 Duplicates: {duplicates_2018}\")\n",
        "print(f\"2019 Duplicates: {duplicates_2019}\")\n"
      ],
      "id": "c60c4dac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b.\n",
        "\n",
        " The identical values in both plots indicate that each short-term hospital appears only once per year, with no duplicates. This structure confirms that each hospital’s CMS certification number is unique annually, allowing for clear year-over-year tracking.\n",
        "\n",
        "## Identify hospital closures in POS file (15 pts) (*)\n",
        "\n",
        "1. \n"
      ],
      "id": "dfd310eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Filter active hospitals in each year\n",
        "active_2016 = data_2016[data_2016['PGM_TRMNTN_CD'] == 0]\n",
        "active_2017 = data_2017[data_2017['PGM_TRMNTN_CD'] == 0]\n",
        "active_2018 = data_2018[data_2018['PGM_TRMNTN_CD'] == 0]\n",
        "active_2019 = data_2019[data_2019['PGM_TRMNTN_CD'] == 0]\n",
        "\n",
        "# Step 2: Define function to check if provider is active in a given year\n",
        "def provider_in_year(df, provider_num):\n",
        "    row = df[df['PRVDR_NUM'] == provider_num]\n",
        "    return not row.empty and row['PGM_TRMNTN_CD'].values[0] == 0\n",
        "\n",
        "# Step 3: Determine closure year for each provider number\n",
        "def determine_closure_year(provider_num):\n",
        "    if not provider_in_year(active_2017, provider_num):\n",
        "        return 2017\n",
        "    elif not provider_in_year(active_2018, provider_num):\n",
        "        return 2018\n",
        "    elif not provider_in_year(active_2019, provider_num):\n",
        "        return 2019\n",
        "    return None\n",
        "\n",
        "# Step 4: Apply closure detection on active hospitals in 2016\n",
        "active_2016_only = active_2016[active_2016['PGM_TRMNTN_CD'] == 0]\n",
        "active_2016_only['Year_Closed'] = active_2016_only['PRVDR_NUM'].apply(determine_closure_year)\n",
        "\n",
        "# Step 5: Filter out hospitals that have a closure year assigned\n",
        "closed_hospitals = active_2016_only.dropna(subset=['Year_Closed']).reset_index(drop=True)\n",
        "\n",
        "# Output the result\n",
        "print(f\"Number of suspected hospital closures: {len(closed_hospitals)}\")\n",
        "closed_hospitals[['FAC_NAME', 'ZIP_CD', 'Year_Closed']]"
      ],
      "id": "7df1b59c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "af76349a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sort the closed hospitals by facility name\n",
        "sorted_closed_hospitals = closed_hospitals.sort_values(by='FAC_NAME').reset_index(drop=True)\n",
        "\n",
        "# Display the names and year of suspected closure for the first 10 rows\n",
        "sorted_closed_hospitals[['FAC_NAME', 'Year_Closed']].head(10)"
      ],
      "id": "e387d018",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "    a.\n"
      ],
      "id": "b3916364"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Count active hospitals by ZIP code for each year\n",
        "active_by_zip_2016 = active_2016['ZIP_CD'].value_counts().to_dict()\n",
        "active_by_zip_2017 = active_2017['ZIP_CD'].value_counts().to_dict()\n",
        "active_by_zip_2018 = active_2018['ZIP_CD'].value_counts().to_dict()\n",
        "active_by_zip_2019 = active_2019['ZIP_CD'].value_counts().to_dict()\n",
        "\n",
        "# Step 2: Function to check if closure might be due to merger/acquisition\n",
        "def is_possible_merger(row):\n",
        "    zip_code = row['ZIP_CD']\n",
        "    year_closed = row['Year_Closed']\n",
        "    \n",
        "    if year_closed == 2017:\n",
        "        return active_by_zip_2016.get(zip_code, 0) <= active_by_zip_2017.get(zip_code, 0)\n",
        "    elif year_closed == 2018:\n",
        "        return active_by_zip_2017.get(zip_code, 0) <= active_by_zip_2018.get(zip_code, 0)\n",
        "    elif year_closed == 2019:\n",
        "        return active_by_zip_2018.get(zip_code, 0) <= active_by_zip_2019.get(zip_code, 0)\n",
        "    return False\n",
        "\n",
        "# Step 3: Apply the function to identify potential mergers/acquisitions\n",
        "closed_hospitals['Possible_Merger'] = closed_hospitals.apply(is_possible_merger, axis=1)\n",
        "\n",
        "# Filter the suspected closures that might be due to a merger/acquisition\n",
        "potential_mergers = closed_hospitals[closed_hospitals['Possible_Merger']]\n",
        "\n",
        "# Output the result\n",
        "num_potential_mergers = len(potential_mergers)\n",
        "print(f\"Number of suspected hospital closures that fit the merger/acquisition definition: {num_potential_mergers}\")"
      ],
      "id": "4c0e3cba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b.\n"
      ],
      "id": "c9f4e927"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the corrected number of closures after removing mergers/acquisitions\n",
        "corrected_closures = len(closed_hospitals) - num_potential_mergers\n",
        "print(f\"Number of hospitals remaining after correcting for mergers/acquisitions: {corrected_closures}\")"
      ],
      "id": "07053f65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    c.\n"
      ],
      "id": "6f0b18af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter out the potential mergers/acquisitions to get the corrected closures\n",
        "corrected_closures = closed_hospitals[~closed_hospitals['Possible_Merger']]\n",
        "\n",
        "# Sort the corrected closures by facility name\n",
        "sorted_corrected_closures = corrected_closures.sort_values(by='FAC_NAME')\n",
        "\n",
        "# Display the first 10 rows\n",
        "sorted_corrected_closures[['FAC_NAME', 'ZIP_CD', 'Year_Closed']].head(10)"
      ],
      "id": "39fe2874",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Census zip code shapefile (10 pt) \n",
        "\n",
        "1. \n",
        "    a.\n",
        "\n",
        ".shp: Main file with geometric shapes of features.\n",
        ".shx: Index file for quick access to shapes.\n",
        ".dbf: Attribute data in table format.\n",
        ".prj: Projection information for the coordinate system.\n",
        ".xml: Metadata about the dataset.\n",
        "\n",
        "\n",
        "\n",
        "    b. "
      ],
      "id": "8a999b72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "# Replace 'shapefile_directory' with the path to your extracted shapefile directory\n",
        "shapefile_directory = \"/Users/attaullah/Documents/problem-set-4-atta/gz_2010_us_860_00_500k\"\n",
        "\n",
        "# Get a list of files and their sizes\n",
        "for file_name in os.listdir(shapefile_directory):\n",
        "    file_path = os.path.join(shapefile_directory, file_name)\n",
        "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    print(f\"{file_name}: {size_mb:.2f} MB\")"
      ],
      "id": "d185fe8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This provides an overview of file sizes, with .shp being the largest due to detailed geometry data.\n",
        "\n",
        "2. \n"
      ],
      "id": "518cdaa6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load 2016 POS data and filter for short-term hospitals in Texas\n",
        "hospital_data_path = \"/Users/attaullah/Documents/problem-set-4-atta/pos2016.csv\"\n",
        "pos2016 = pd.read_csv(hospital_data_path)\n",
        "texas_hospitals = pos2016[(pos2016[\"PRVDR_CTGRY_SBTYP_CD\"] == 1) &\n",
        "                          (pos2016[\"PRVDR_CTGRY_CD\"] == 1) &\n",
        "                          (pos2016[\"ZIP_CD\"].astype(str).str.startswith((\"75\", \"76\", \"77\", \"78\", \"79\")))]\n",
        "\n",
        "# Standardize ZIP codes as 5-digit strings\n",
        "texas_hospitals[\"ZIP_CD\"] = texas_hospitals[\"ZIP_CD\"].astype(int).astype(str).str.zfill(5)\n",
        "\n",
        "# Calculate number of hospitals per ZIP code in Texas\n",
        "hospital_counts = texas_hospitals.groupby(\"ZIP_CD\").size().reset_index(name=\"Hospital_Count\")\n",
        "\n",
        "# Load Texas ZIP code shapefile and filter for Texas ZIPs\n",
        "shapefile_path = \"/Users/attaullah/Documents/problem-set-4-atta/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp\"\n",
        "zip_gdf = gpd.read_file(shapefile_path)\n",
        "zip_gdf[\"ZCTA5\"] = zip_gdf[\"ZCTA5\"].astype(str).str.zfill(5)\n",
        "texas_zip_gdf = zip_gdf[zip_gdf[\"ZCTA5\"].str.startswith((\"75\", \"76\", \"77\", \"78\", \"79\"))]\n",
        "\n",
        "# Merge hospital data with Texas ZIP code shapefile\n",
        "merged_gdf = texas_zip_gdf.merge(hospital_counts, left_on=\"ZCTA5\", right_on=\"ZIP_CD\", how=\"left\")\n",
        "merged_gdf[\"Hospital_Count\"] = merged_gdf[\"Hospital_Count\"].fillna(0)\n",
        "\n",
        "# Check the distribution of hospital counts\n",
        "print(merged_gdf['Hospital_Count'].describe())\n",
        "print(merged_gdf['Hospital_Count'].value_counts().head(20))\n",
        "# Plot choropleth map of hospital counts by ZIP code in Texas\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "merged_gdf.plot(\n",
        "    column=\"Hospital_Count\",\n",
        "    cmap=\"OrRd\",\n",
        "    linewidth=0.8,\n",
        "    edgecolor=\"gray\",\n",
        "    legend=True,\n",
        "    legend_kwds={\"label\": \"Hospital Count\"},\n",
        "    ax=ax\n",
        ")\n",
        "ax.set_title(\"Number of Hospitals by ZIP Code in Texas (2016)\", fontsize=15)\n",
        "ax.axis(\"off\")  # Remove axis for cleaner map\n",
        "plt.show()\n"
      ],
      "id": "8d6b483f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate zip code’s distance to the nearest hospital (20 pts) (*)\n",
        "\n",
        "1.\n"
      ],
      "id": "a27cfb7e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Load the ZIP code shapefile\n",
        "shapefile_path = '/Users/attaullah/Documents/problem-set-4-atta/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp'\n",
        "zip_gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Calculate the centroid for each ZIP code polygon\n",
        "# Assuming 'ZCTA5' is the ZIP code column in the shapefile\n",
        "zip_gdf['centroid'] = zip_gdf.geometry.centroid\n",
        "\n",
        "# Create a new GeoDataFrame with centroids\n",
        "zips_all_centroids = gpd.GeoDataFrame(zip_gdf[['ZCTA5', 'centroid']], geometry='centroid')\n",
        "\n",
        "# Print the dimensions of the resulting GeoDataFrame\n",
        "print(\"Dimensions of zips_all_centroids:\", zips_all_centroids.shape)\n",
        "\n",
        "# Display the first few rows to understand the structure\n",
        "print(zips_all_centroids.head())"
      ],
      "id": "923b7dfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The zips_all_centroids GeoDataFrame contains the centroids of each ZIP code area in the U.S., representing the central point of each geographic area.\n",
        "\n",
        "Dimensions: (33120, 2), with 33,120 rows and 2 columns, where each row represents a ZIP code area.\n",
        "\n",
        "Columns:\n",
        "\n",
        "ZCTA5: ZIP Code Tabulation Area (ZIP code) as a string.\n",
        "centroid: Geometric POINT object with latitude and longitude, marking the center of each ZIP code area.\n",
        "\n",
        "2. \n"
      ],
      "id": "af2256ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# Define ZIP code prefixes for Texas and its bordering states\n",
        "texas_prefixes = [\"75\", \"76\", \"77\", \"78\", \"79\"]  # Texas\n",
        "bordering_prefixes = texas_prefixes + [\n",
        "    \"73\", \"74\",              # Oklahoma\n",
        "    \"870\", \"871\", \"872\", \"873\", \"874\", \"875\", \"876\", \"877\", \"878\", \"879\", \"880\", \"881\", \"882\", \"883\", \"884\",  # New Mexico\n",
        "    \"700\", \"701\", \"702\", \"703\", \"704\", \"705\", \"706\", \"707\", \"708\", \"709\", \"710\", \"711\", \"712\", \"713\", \"714\", \"715\",  # Louisiana\n",
        "    \"716\", \"717\", \"718\", \"719\", \"720\", \"721\", \"722\", \"723\", \"724\", \"725\", \"726\", \"727\", \"728\", \"729\"  # Arkansas\n",
        "]\n",
        "\n",
        "# Create subset for Texas ZIP codes\n",
        "zips_texas_centroids = zips_all_centroids[zips_all_centroids[\"ZCTA5\"].str.startswith(tuple(texas_prefixes))]\n",
        "\n",
        "# Create subset for Texas and bordering states ZIP codes\n",
        "zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids[\"ZCTA5\"].str.startswith(tuple(bordering_prefixes))]\n",
        "\n",
        "# Calculate the number of unique ZIP codes in each subset\n",
        "unique_texas_zips = zips_texas_centroids[\"ZCTA5\"].nunique()\n",
        "unique_borderstates_zips = zips_texas_borderstates_centroids[\"ZCTA5\"].nunique()\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of unique ZIP codes in Texas:\", unique_texas_zips)\n",
        "print(\"Number of unique ZIP codes in Texas and bordering states:\", unique_borderstates_zips)\n",
        "\n",
        "# Display the first few rows of each subset to verify\n",
        "print(\"Sample of Texas ZIP codes:\\n\", zips_texas_centroids.head())\n",
        "print(\"Sample of Texas and bordering states ZIP codes:\\n\", zips_texas_borderstates_centroids.head())"
      ],
      "id": "cced84c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output shows:\n",
        "Number of unique ZIP codes in Texas: 1935\n",
        "Number of unique ZIP codes in Texas and bordering states: 4057\n",
        "\n",
        "3. \n"
      ],
      "id": "fd1e5f45"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "# Assume we already have `zips_texas_borderstates_centroids` created and `hospital_counts` data from the previous part.\n",
        "\n",
        "# Filter hospital data to include only ZIP codes with at least one hospital in 2016\n",
        "# 'hospital_counts' should be the dataframe that contains ZIP codes and counts of hospitals in each ZIP code\n",
        "hospital_counts_2016 = hospital_counts[hospital_counts['Hospital_Count'] > 0]\n",
        "\n",
        "# Perform an inner merge on 'ZCTA5' (ZIP code) to get only those ZIP codes with at least one hospital\n",
        "zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(\n",
        "    hospital_counts_2016, \n",
        "    left_on=\"ZCTA5\", \n",
        "    right_on=\"ZIP_CD\", \n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Check the resulting GeoDataFrame\n",
        "print(\"Dimensions of zips_withhospital_centroids:\", zips_withhospital_centroids.shape)\n",
        "print(zips_withhospital_centroids.head())"
      ],
      "id": "e18101bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge Type: I used an inner join to ensure that the resulting zips_withhospital_centroids GeoDataFrame only includes ZIP codes that have at least one hospital in 2016 and are also present in the zips_texas_borderstates_centroids GeoDataFrame.\n",
        "\n",
        "Merge Variable: The merge is performed on the ZCTA5 column from zips_texas_borderstates_centroids and ZIP_CD from hospital_counts_2016, which represent ZIP codes in both datasets.\n",
        "\n",
        "4. \n",
        "    a.\n"
      ],
      "id": "6e7883da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reproject both GeoDataFrames to a projected CRS (e.g., EPSG:5070 for USA)\n",
        "zips_texas_centroids = zips_texas_centroids.to_crs(epsg=5070)\n",
        "zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=5070)\n",
        "\n",
        "# Re-run the distance calculation with the reprojected GeoDataFrames\n",
        "import time\n",
        "\n",
        "# Subset to 10 ZIP codes in Texas\n",
        "subset_texas_centroids = zips_texas_centroids.iloc[:10]\n",
        "\n",
        "# Start timing the process\n",
        "start_time = time.time()\n",
        "\n",
        "# Calculate the nearest hospital distance for each ZIP code in the subset\n",
        "subset_texas_centroids['nearest_hospital_distance'] = subset_texas_centroids['centroid'].apply(\n",
        "    lambda x: zips_withhospital_centroids.distance(x).min()\n",
        ")\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "subset_time = end_time - start_time\n",
        "print(\"Time taken for 10 ZIP code calculations:\", subset_time, \"seconds\")\n",
        "\n",
        "# Estimate time for full calculation\n",
        "estimated_total_time = subset_time * (len(zips_texas_centroids) / 10)\n",
        "print(\"Estimated time for the full procedure:\", estimated_total_time, \"seconds\")\n",
        "\n",
        "# Display the results for the subset\n",
        "print(subset_texas_centroids[['ZCTA5', 'nearest_hospital_distance']])"
      ],
      "id": "780d620a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b.\n"
      ],
      "id": "a3237e17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Start the timer for the full calculation on zips_texas_centroids\n",
        "start_time_full = time.time()\n",
        "\n",
        "# Apply the distance calculation for each ZIP code in Texas to the nearest ZIP code with a hospital\n",
        "zips_texas_centroids[\"nearest_hospital_distance\"] = zips_texas_centroids[\"centroid\"].apply(\n",
        "    lambda x: zips_withhospital_centroids.distance(x).min()\n",
        ")\n",
        "\n",
        "# End the timer\n",
        "end_time_full = time.time()\n",
        "actual_time_full = end_time_full - start_time_full\n",
        "\n",
        "# Print out the actual time taken\n",
        "print(f\"Actual time for full calculation: {actual_time_full} seconds\")"
      ],
      "id": "fb7ba22d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The actual time of 0.31 seconds is quite close to the estimated time of 0.66 seconds, showing the estimation was reasonably accurate.\n",
        "\n",
        "    c. \n",
        "\n",
        "load and read the contents of the .prj file, which provides the geographic coordinate system (GCS) details:\n"
      ],
      "id": "d5206acc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Path to the .prj file\n",
        "prj_file_path = '/Users/attaullah/Documents/problem-set-4-atta/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.prj'\n",
        "\n",
        "# Read and print the .prj file contents\n",
        "with open(prj_file_path, 'r') as file:\n",
        "    prj_contents = file.read()\n",
        "\n",
        "print(\"Contents of the .prj file:\")\n",
        "print(prj_contents)\n"
      ],
      "id": "54aa4dfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the unit in the dataset is in degrees, I used an approximation to convert degrees to miles at a latitude of 30° (Texas' approximate latitude). For conversions, it’s commonly estimated that 1 degree of latitude equals roughly 69 miles, while 1 degree of longitude varies based on latitude. At 30° latitude, the conversion factor for longitude is approximately 69.172 miles.\n",
        "\n",
        "This conversion method is based on information provided at: https://gis.stackexchange.com/questions/142326/calculating-longitude-length-in-miles\n",
        "\n",
        "Using these estimates, I applied an average of latitude and longitude conversions to calculate the distance in miles:\n"
      ],
      "id": "c8db8589"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "\n",
        "# Function to convert distance in degrees to miles at a given latitude (approx. for Texas)\n",
        "def degrees_to_miles(distance_in_degrees, latitude=30):\n",
        "    # Convert latitude to radians\n",
        "    latitude_in_radians = math.radians(latitude)\n",
        "    \n",
        "    # Approximate miles per degree for latitude and longitude\n",
        "    miles_per_degree_latitude = 69.0\n",
        "    miles_per_degree_longitude = 69.172 * math.cos(latitude_in_radians)\n",
        "    \n",
        "    # Average of latitude and longitude mile conversion for more accuracy\n",
        "    avg_miles_per_degree = (miles_per_degree_latitude + miles_per_degree_longitude) / 2\n",
        "    return distance_in_degrees * avg_miles_per_degree\n",
        "\n",
        "# Use the computed average distance in degrees from the dataset\n",
        "distance_in_degrees = 0.0753484763  # Computed average distance in degrees\n",
        "distance_in_miles = degrees_to_miles(distance_in_degrees)\n",
        "print(\"Average distance to nearest hospital (in miles):\", distance_in_miles)"
      ],
      "id": "13f4de8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. \n",
        "    a.\n"
      ],
      "id": "9085c980"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the nearest hospital distance in degrees\n",
        "zips_texas_centroids[\"nearest_hospital_distance_degrees\"] = zips_texas_centroids[\"centroid\"].apply(\n",
        "    lambda x: zips_withhospital_centroids.distance(x).min()\n",
        ")\n",
        "\n",
        "# Calculate the average distance in degrees\n",
        "average_distance_degrees = zips_texas_centroids[\"nearest_hospital_distance_degrees\"].mean()\n",
        "print(\"Average distance to the nearest hospital for each ZIP code in Texas (in degrees):\", average_distance_degrees)"
      ],
      "id": "f9030446",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is in Degrees. \n",
        "\n",
        "\n",
        "    b.  \n"
      ],
      "id": "ae24a631"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n",
        "\n",
        "# Function to convert degrees to miles based on approximate latitude for Texas (30 degrees)\n",
        "def degrees_to_miles(distance_in_degrees, latitude=30):\n",
        "    latitude_in_radians = math.radians(latitude)\n",
        "    miles_per_degree_latitude = 69.0  # Approximation for latitude degree\n",
        "    miles_per_degree_longitude = 69.172 * math.cos(latitude_in_radians)  # Approximation for longitude degree\n",
        "    avg_miles_per_degree = (miles_per_degree_latitude + miles_per_degree_longitude) / 2\n",
        "    return distance_in_degrees * avg_miles_per_degree\n",
        "\n",
        "# Use the previously calculated average distance in degrees\n",
        "distance_in_miles = degrees_to_miles(average_distance_degrees)\n",
        "print(\"Average distance to the nearest hospital for each ZIP code in Texas (in miles):\", distance_in_miles)\n"
      ],
      "id": "b979fa78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reproject to a projected CRS for accurate distance calculation\n",
        "zips_texas_centroids = zips_texas_centroids.to_crs(epsg=3395)  # World Mercator\n",
        "zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=3395)\n",
        "\n",
        "# Recalculate nearest hospital distance\n",
        "zips_texas_centroids[\"nearest_hospital_distance_meters\"] = zips_texas_centroids[\"centroid\"].apply(\n",
        "    lambda x: zips_withhospital_centroids.distance(x).min()\n",
        ")\n",
        "\n",
        "# Convert the distance from meters to miles\n",
        "zips_texas_centroids[\"nearest_hospital_distance_miles\"] = zips_texas_centroids[\"nearest_hospital_distance_meters\"] / 1609.34\n",
        "\n",
        "# Calculate the average distance in miles\n",
        "average_distance_miles_corrected = zips_texas_centroids[\"nearest_hospital_distance_miles\"].mean()\n",
        "print(\"Corrected average distance to nearest hospital (in miles):\", average_distance_miles_corrected)"
      ],
      "id": "a5789cf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The initial result of 996,695.26 miles for the average distance to the nearest hospital was clearly unrealistic. This suggested an issue with unit conversion.\n",
        "\n",
        "After re-projecting the data for accurate distance measurements, the corrected average was 9.61 miles, which aligns well with realistic hospital distribution in Texas. This correction provided a reasonable and accurate measure of the average distance to the nearest hospital.\n",
        "\n",
        "    c.\n"
      ],
      "id": "7d366fb2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the choropleth map for average distance to nearest hospital in Texas\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "zips_texas_centroids.plot(\n",
        "    column='nearest_hospital_distance_miles',  # Column with the distance in miles\n",
        "    cmap='viridis',                            # Color map\n",
        "    linewidth=0.8,                             # Border width for ZIP code polygons\n",
        "    ax=ax,\n",
        "    edgecolor='0.8',                           # Color for polygon borders\n",
        "    legend=True,\n",
        "    legend_kwds={'label': \"Distance to Nearest Hospital (miles)\"}\n",
        ")\n",
        "\n",
        "# Adding map title and turning off axis for clarity\n",
        "ax.set_title(\"Average Distance to Nearest Hospital by ZIP Code in Texas\", fontsize=15)\n",
        "ax.axis('off')  # Hides the axis\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "3695f300",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Effects of closures on access in Texas (15 pts)\n",
        "\n",
        "1. \n"
      ],
      "id": "4d979395"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert ZIP_CD to string, remove any '.0' suffix if present, then ensure it's a 5-digit string\n",
        "closed_hospitals['ZIP_CD'] = closed_hospitals['ZIP_CD'].astype(str).str.replace(r'\\.0$', '', regex=True).str.zfill(5)\n",
        "\n",
        "# Filter the closures data for ZIP codes in Texas (based on known Texas ZIP code prefixes)\n",
        "texas_closures = closed_hospitals[closed_hospitals['ZIP_CD'].str.startswith(('75', '76', '77', '78', '79'))]\n",
        "\n",
        "# Group by ZIP code and count closures\n",
        "closures_by_zipcode = texas_closures.groupby('ZIP_CD').size().reset_index(name='Closure_Count')\n",
        "\n",
        "# Display the table of closures by ZIP code\n",
        "print(\"Table of Hospital Closures by ZIP Code in Texas (2016-2019):\")\n",
        "print(closures_by_zipcode)\n",
        "\n",
        "# Optionally display the top affected ZIP codes for brevity\n",
        "print(\"Top Affected ZIP Codes by Hospital Closures:\")\n",
        "print(closures_by_zipcode.sort_values(by='Closure_Count', ascending=False).head(10))"
      ],
      "id": "5ae1e881",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "ef8d77cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "\n",
        "# Load Texas ZIP codes shapefile and hospital closures data\n",
        "shapefile_path = \"/Users/attaullah/Documents/problem-set-4-atta/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp\"  # Adjust the path\n",
        "zip_gdf = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Define Texas ZIP code prefixes and filter the data\n",
        "texas_zip_prefixes = [\"75\", \"76\", \"77\", \"78\", \"79\"]\n",
        "zip_gdf[\"ZCTA5\"] = zip_gdf[\"ZCTA5\"].astype(str)\n",
        "texas_zip_gdf = zip_gdf[zip_gdf[\"ZCTA5\"].str.startswith(tuple(texas_zip_prefixes))]\n",
        "\n",
        "# Merge Texas ZIP codes with closure data\n",
        "closures_by_zipcode['ZIP_CD'] = closures_by_zipcode['ZIP_CD'].astype(str).str.zfill(5)\n",
        "texas_closure_geo = texas_zip_gdf.merge(closures_by_zipcode, left_on=\"ZCTA5\", right_on=\"ZIP_CD\", how=\"left\")\n",
        "texas_closure_geo['Closure_Count'] = texas_closure_geo['Closure_Count'].fillna(0)\n",
        "\n",
        "# Plot the map\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "texas_closure_geo.plot(\n",
        "    column='Closure_Count',\n",
        "    cmap='OrRd',\n",
        "    linewidth=0.8,\n",
        "    ax=ax,\n",
        "    edgecolor='0.8',\n",
        "    legend=True\n",
        ")\n",
        "ax.set_title(\"Texas ZIP Codes Directly Affected by Hospital Closures (2016-2019)\")\n",
        "ax.set_axis_off()\n",
        "plt.show()\n"
      ],
      "id": "6562a0c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n"
      ],
      "id": "b0c0df96"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-project to a projected CRS for accurate buffering\n",
        "texas_zip_gdf = texas_zip_gdf.to_crs(epsg=3395)\n",
        "directly_affected_zips = texas_closure_geo[texas_closure_geo['Closure_Count'] > 0].to_crs(epsg=3395)\n",
        "\n",
        "# Step 1: Create a 10-mile buffer around directly affected ZIP codes (10 miles ≈ 16093.4 meters)\n",
        "directly_affected_zips['buffer'] = directly_affected_zips.geometry.buffer(16093.4)  # Buffer in meters\n",
        "\n",
        "# Step 2: Convert buffered areas to a new GeoDataFrame\n",
        "buffered_zips = gpd.GeoDataFrame(directly_affected_zips[['ZIP_CD', 'buffer']], geometry='buffer', crs=directly_affected_zips.crs)\n",
        "\n",
        "# Step 3: Plot Texas ZIP codes and buffer zones for verification (optional visualization)\n",
        "ax = texas_zip_gdf.plot(color='lightgrey', edgecolor='black', figsize=(10, 8))\n",
        "buffered_zips.plot(ax=ax, color='none', edgecolor='red')  # Buffer zones in red outline\n",
        "plt.title(\"Texas ZIP Codes and 10-Mile Buffers Around Directly Affected ZIP Codes\")\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Perform spatial join to identify indirectly affected ZIP codes within buffer areas\n",
        "indirectly_affected_zips = gpd.sjoin(texas_zip_gdf, buffered_zips, how='inner', predicate='intersects')\n",
        "\n",
        "# Step 5: Count unique ZIP codes in the indirectly affected set\n",
        "indirectly_affected_count = indirectly_affected_zips['ZIP_CD'].nunique()\n",
        "print(\"Number of indirectly affected ZIP codes:\", indirectly_affected_count)"
      ],
      "id": "382be3ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n"
      ],
      "id": "9b1f782b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "\n",
        "# Step 1: Re-project data for accuracy (if not already done)\n",
        "texas_zip_gdf = texas_zip_gdf.to_crs(epsg=3395)\n",
        "directly_affected_zips = texas_closure_geo[texas_closure_geo['Closure_Count'] > 0].to_crs(epsg=3395)\n",
        "\n",
        "# Step 2: Create a 10-mile buffer around directly affected ZIP codes\n",
        "directly_affected_zips['buffer'] = directly_affected_zips.geometry.buffer(16093.4)  # Buffer in meters\n",
        "buffered_zips = gpd.GeoDataFrame(directly_affected_zips[['ZCTA5', 'buffer']], geometry='buffer', crs=directly_affected_zips.crs)\n",
        "\n",
        "# Step 3: Identify indirectly affected ZIP codes\n",
        "indirectly_affected_zips = gpd.sjoin(texas_zip_gdf, buffered_zips, how='inner', predicate='intersects')\n",
        "indirectly_affected_zip_codes = indirectly_affected_zips['ZCTA5_left'].unique()  # Use 'ZCTA5_left' from join result\n",
        "\n",
        "# Step 4: Classify ZIP codes into categories\n",
        "texas_zip_gdf['category'] = 'Not Affected'\n",
        "texas_zip_gdf.loc[texas_zip_gdf['ZCTA5'].isin(directly_affected_zips['ZCTA5']), 'category'] = 'Directly Affected'\n",
        "texas_zip_gdf.loc[(texas_zip_gdf['ZCTA5'].isin(indirectly_affected_zip_codes)) & \n",
        "                  (~texas_zip_gdf['ZCTA5'].isin(directly_affected_zips['ZCTA5'])), 'category'] = 'Indirectly Affected'\n",
        "\n",
        "# Step 5: Plot the choropleth\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
        "category_colors = {'Not Affected': 'lightgrey', 'Directly Affected': 'red', 'Indirectly Affected': 'orange'}\n",
        "texas_zip_gdf.plot(column='category', categorical=True, \n",
        "                   legend=True, color=[category_colors.get(x) for x in texas_zip_gdf['category']],\n",
        "                   legend_kwds={'title': \"Impact Category\"}, ax=ax, edgecolor='black')\n",
        "\n",
        "# Adding a title and displaying the plot\n",
        "plt.title(\"Texas ZIP Codes Categorized by Impact of Hospital Closures (2016-2019)\")\n",
        "plt.axis('off')  # Hide axis for a cleaner look\n",
        "plt.show()"
      ],
      "id": "4eb19240",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflecting on the exercise (10 pts) \n",
        "\n",
        "1. \n",
        "The first-pass method risks misidentifying closures because it doesn’t account for temporary closures, facility mergers, or changes in hospital IDs. This can lead to over- or under-counting affected areas. To improve, we could:\n",
        "\n",
        "Cross-reference closures with state and local health department data to verify permanent status.\n",
        "Track hospital utilization trends over time to differentiate between closures and service reductions.\n",
        "Use multiple years of data to confirm if facilities remain closed rather than reopening under new identifiers.\n",
        "Integrate local news reports and community health resources to validate closures.\n",
        "These steps could make the identification process more accurate and reduce misclassification.\n",
        "\n",
        "\n",
        "2. \n",
        "\n",
        "The current approach, identifying affected ZIP codes based on a 10-mile radius around closures, provides a rough estimate but doesn’t fully capture changes in access. Here are some ways to improve:\n",
        "\n",
        "Travel Time Analysis: Use drive time rather than straight-line distance, as actual access depends on road networks, traffic, and transportation options.\n",
        "\n",
        "Population Density and Demand: Factor in population density and local demand to assess how many people are impacted within each affected ZIP code.\n",
        "\n",
        "Alternative Facilities: Account for nearby hospitals or healthcare facilities that could serve as substitutes, providing a more realistic measure of lost access.\n",
        "\n",
        "Demographic and Socioeconomic Factors: Include variables like age, income, and car ownership to reflect differences in accessibility and reliance on local hospitals across communities.\n",
        "\n",
        "These adjustments could give a more accurate picture of how hospital closures impact healthcare access at the ZIP-code level."
      ],
      "id": "1d70dd78"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/attaullah/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}